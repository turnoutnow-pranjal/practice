 Bonjour et bienvenue à choisir la base de données adaptée aux applications modernes dans le cadre de notre nouvelle série en ligne « West Summit ^0. Je m'appelle William Wong et je suis un architecte spécialisé en solutions de bases de données ^1. Aujourd'hui, je serai rejoint par Michael Riccardi, notre architecte spécialisé en solutions de développement ^2. Aujourd'hui ^3. Nous sommes très heureux d'être ici pour vous montrer comment des bases de données spécialement conçues peuvent être utilisées pour améliorer l'échelle, les performances et la disponibilité de vos applications. Notre agenda présentera les exigences des applications modernes ^4. Je vais ensuite examiner les défis qui sont résolus à l'aide de microservices et de bases de données spécialisées. Avant d'examiner comment nous pouvons sélectionner les bases de données adaptées à vos charges de travail particulières ^5. Michael ^6. Nous allons ensuite approfondir la question, où il modernisera une application, passant d'une pile de bases de données relationnelles monolithique à une application utilisant des bases de données spécialement conçues, et vous montrera les différences en termes de performances et d'échelle ^7. Commençons donc et examinons les exigences de nos applications modernes ^8. Et nous examinons d'abord les applications que nous utilisons tous au quotidien, comme le covoiturage, les médias, le streaming, les services bancaires, les jeux vidéo et les réseaux sociaux ^9. Nous commençons à voir quelques modèles courants ^10. Ces modèles peuvent être des millions d'utilisateurs accédant à nos applications dans différentes zones géographiques ^11. Et ces mêmes utilisateurs s'attendent à des expériences instantanées qui pourraient se résumer à des temps de réponse constants en millisecondes, voire en moins de la milliseconde ^12. Nos applications devront être surdimensionnées pour répondre aux demandes liées à des événements tels que nos ventes flash ou le traitement de l'urine, puis être réduites à la baisse ^13. Nous ne sommes pas utilisés afin de minimiser les coûts ^14. Il est prévu de recueillir davantage de données au cours des trois prochaines années que celles des 30 dernières années ^15. Et cela nécessitera des outils spécialisés pour traiter les pétaoctets ^16. Si ce n'est pas zéro octet de données structurées et non structurées ^17. Si nous prenons le temps de réfléchir aux modèles architecturaux de la plupart de ces applications basées sur le cloud, vous constaterez qu'il s'agit toutes de microservices ^18. Cela signifie qu'ils sont hautement distribués, faiblement couplés et accessibles via un P ^19. I ^20. S ^21. Que signifie donc ce changement d'architecture pour notre base de données sous-jacente alors que nos applications sont désormais découplées en services ^22. Il permet à chacun de ces services de disposer de sa propre base de données indépendante ^23. Et cela nous apportera de nombreux avantages, par exemple, nous donnera la possibilité d'hyperdimensionner nos applications, car chaque service et canal de base de données évolue indépendamment ^24. Prenons le Black Friday comme cas d'utilisation ^25. Il se peut que nous devions adapter notre catalogue et notre service de paiement à des centaines de milliers de demandes simultanées sur une courte période, mais le nombre de nos inscriptions d'utilisateurs risque de ne pas augmenter au même degré ^26. Un autre avantage est l'agilité supplémentaire ^27. Puisque nous pouvons désormais innover plus rapidement sur différents composants ^28. Nous pouvons rapidement tester et restaurer de nouvelles versions et fonctionnalités à un niveau modulaire sans avoir à gérer toutes les dépendances couplées complexes liées aux monolithes ^29. L'un des défis commerciaux courants consiste à rendre nos applications plus disponibles et, en découplant nos bases de données, nous augmenterons leur disponibilité globale, étant donné que nous ne disposons plus d'une base de données monolithique qui sert de point de conflit unique pour nos événements tels que les déploiements de code ou les mises à niveau et les correctifs ^30. Maintenant que nous comprenons les avantages du découplage de nos données ^31. Pourquoi envisagerions-nous des bases de données spécialement conçues pour nos microservices ? Nos développeurs veulent disposer de la base de données adaptée aux besoins de nos applications modernes, comme nous l'avons décrit précédemment ^32. Et franchement, cette approche universelle qui consiste à utiliser une base de données relationnelle pour tout ne fonctionne tout simplement plus ^33. Par exemple, nous pouvons avoir besoin d'une base de données pour fournir des temps de réponse de latence de l'ordre de la microseconde afin de pouvoir afficher rapidement nos sites Web ou pour fournir des temps de réponse cohérents afin de répondre aux demandes des utilisateurs d'un chirurgien ^34. Et l'architecture relationnelle n'est pas la mieux adaptée à ces cas d'utilisation particuliers ^35. En fait, le fait de ne pas utiliser la bonne base de données entraîne généralement des problèmes de performances, un manque d'évolutivité, un manque de flexibilité pour les développeurs et une augmentation de nos coûts globaux ^36. Donc, traditionnellement, lorsque je parle à mes clients, je me heurte à un obstacle à l'adoption de ces bases de données spécialement conçues avec des frais généraux opérationnels potentiels ^37. Cela comprenait des investissements dans du matériel et des logiciels initiaux ou une expertise pour les rendre évolutifs, hautement disponibles et performants ^38. Et c'est là qu'AWS entre en jeu ^39. Nous proposerons le portefeuille le plus large et le plus complet de plus de 15 bases de données spécialisées capables de prendre en charge les différents modèles de données en exploitant les bases de données personnalisées et entièrement gérées que nous avons créées à partir de zéro. Les clients peuvent désormais gagner du temps et de l'argent, améliorer les performances à grande échelle et innover plus rapidement ^40. Nous avons créé des bases de données spécialement conçues pour répondre à tous les cas d'utilisation, comme le document à valeur clé relationnelle en mémoire, le registre des séries chronologiques à colonnes larges et nos bases de données graphiques ^41. Maintenant que nous avons compris la nécessité de disposer de bases de données spécialement conçues, comment procéderiez-vous pour sélectionner les bases de données adaptées à vos cas d'utilisation particuliers ? Ce que j'aimerais dire aux gens, c'est qu'au lieu de regarder une liste de 100 bases de données différentes, pourquoi ne pas commencer par réfléchir à des catégories de bases de données communes ? Maintenant, nous examinons rapidement les catégories ^42. Non seulement vous y trouverez cette base de données relationnelle familière sur la gauche, mais vous verrez également d'autres bases de données, comme notre base de données de documents Amazon qui est optimisée pour stocker les données au format Jason ^43. Et comme il s'agit d'une base de données sans suite, elle nous donnera la flexibilité nécessaire pour modifier les schémas d'applications, mais elle nous permettra également d'interroger des documents en fonction de n'importe quel attribut ^44. Et c'est très pratique pour notre gestion de contenu, nos applications mobiles ou une base de données graphique comme Amazon Neptune, qui nous permet ensuite de travailler avec des ensembles de données hautement connectés ^45. Nous pouvons essayer de modéliser cela dans une base de données relationnelle avec des articulations complexes et des requêtes imbriquées, mais notre latence augmentera à mesure que le nombre de nos relations augmentera dans les bases de données graphiques ^46. Cependant, cela nous permet de parcourir des millions de relations en quelques secondes, ce qui est idéal pour la détection des fraudes, les réseaux sociaux et nos moteurs de recommandation ^47. Enfin, comme une base de données de séries chronologiques comme notre flux temporel Amazon, qui est optimisée pour ingérer des milliards de données de séquence temporelle par jour et nous fournir des fonctions basées sur le temps telles que la corrélation et l'interpolation afin que nous puissions obtenir de meilleures informations à partir de ces données ^48. Et c'est idéal pour nos applications de développement IoT ou de suivi d'événements ^49. Prenons maintenant le temps d'examiner un peu plus en détail certaines catégories très courantes que j'ai observées chez mes clients ^50. Commençons par un modèle de données relationnelles très familier. Les données relationnelles sont hautement structurées, les données sont divisées en tables et les relations imposées par le système sont des clés primaires et référentielles et de bons cas d'utilisation seront les charges de travail pour lesquelles nous ne pouvons pas prédéfinir tous nos modèles d'accès avant ou si nous avions des applications qui nécessitent une intégrité référentielle élevée et une forte cohérence ^51. Comme pour nos systèmes de paiement en ligne ^52. Amazon Arora ^53 est une base de données native dans le cloud que nous pouvons choisir pour nos modèles relationnels. Arora est compatible à la fois avec ma suite et avec Postgres et peut nous aider à améliorer nos performances en fournissant un débit jusqu'à cinq fois supérieur à celui du standard ^54. Ma suite et trois fois plus que celle du standard Postgres ^55. Cela nous aidera à dimensionner automatiquement les ressources de calcul et de stockage et nous avons conçu Billy pour stocker nos données de six manières dans trois zones de disponibilité ^56. Et comme il s'agit d'un service géré, il permettra d'automatiser des tâches telles que le déploiement et le provisionnement, des correctifs et des mises à niveau réguliers, des sauvegardes et nous fournira des fonctionnalités de sécurité telles que le chiffrement en transit et au repos ^57. Une autre excellente catégorie ^58. Pour consulter nos bases de données de valeurs clés ^59. Les données de valeur clé utilisent cette méthode simple pour stocker et récupérer des données. Leur point fort réside dans leur conception qui permet de partitionner ou de partager des données de manière importante, puis de les stocker physiquement en fonction de cette clé de partition ^60. Cette conception lui permet de s'adapter horizontalement à pratiquement n'importe quelle taille tout en nous offrant des temps de réponse constants, quelle que soit l'échelle ^61. Passons donc en revue un cas d'utilisation d'un jeu en ligne qui doit stocker les données de session d'un utilisateur et qui possède un modèle d'accès défini via une balise gamer pour récupérer cet ensemble de données ^62. Comme il s'agit d'un jeu en ligne, nous voulons garantir une expérience constante au joueur, qu'il y ait 10 ou même 100 000 utilisateurs ^63. Si le jeu prend son envol dans ce cas d'utilisation particulier, nous utiliserons une base de données de valeurs clés pour obtenir des temps de réponse cohérents, quelle que soit la croissance de l'application ^64. Amazon Dynamodb ^65 est une base de données spécialement conçue que nous pouvons choisir pour nos bases de données de valeurs clés. Il est entièrement géré et sans serveur, ce qui signifie qu'il se chargera de tous les logiciels de provisionnement, de correctifs de sécurité et, pour nous aider à faire évoluer automatiquement, dynamodb nous permet de créer des applications capables de fournir des temps de réponse à une milliseconde à grande échelle et pour nos applications critiques que nous pourrions utiliser à l'échelle mondiale. réplication pour répliquer nos ensembles de données dans plusieurs régions ^66. Une catégorie très populaire parmi mes clients concerne les bases de données en mémoire. Les bases de données en mémoire stockent nos données en mémoire plutôt que sur celle-ci, ce qui nous donne des temps de réponse inférieurs à la milliseconde ^67. Et c'est idéal pour les cas d'utilisation visant à stocker les données de session d'un utilisateur sur le maintien d'un classement pour les jeux en ligne ou l'entraînement de modèles d'apprentissage automatique ^68. Mais le cas d'utilisation le plus courant que j'ai vu chez mes clients est de placer ici un cash devant nos bases de données relationnelles ^69. Cela nous donnera des temps de réponse bien meilleurs pour nos utilisateurs finaux et atténuera les pressions d'échelle en lecture sur la base de données relationnelle sous-jacente, ce qui peut entraîner des problèmes de blocage des ressources et de stabilité, que Makayla vous montrera dans sa démonstration suivante pour les bases de données en mémoire ^70. Nous avons le choix entre Amazon Elastic Cache ^71. Il s'agit d'un service entièrement géré qui nous permet de fournir des temps de réponse inférieurs à la milliseconde et une mise à l'échelle ^72 sans interruption de service. Il est à la fois compatible avec mem cache ^73. DM Reedus et les personnes qui l'utilisaient pour le cache mém l'utiliseront généralement pour créer des couches de cations simples et évolutives pour leurs applications sensibles à la latence, tandis que nos utilisateurs de Reddit l'utiliseront pour des cas d'utilisation plus polyvalents, tels que pour leurs jeux, leurs analyses en temps réel et leur apprentissage automatique avec Regis ^74. Nous avons également le choix d'Amazon Memory Day Bay, qui est un cluster entièrement géré avec une durabilité de ^75. Memory Debate nous fournit des performances ultrarapides et une compatibilité totale avec Regis open source, ce qui nous permet ensuite d'utiliser toutes ces structures de données riches, telles que des ensembles triés pour les classements en ligne en tirant parti de ^76. Un journal des transactions distribué dans les zones de disponibilité ^77. Memory DB nous permet de restaurer rapidement la base de données et de redémarrer sans risque de perte de données, ce qui signifie que nous pouvons désormais l'utiliser comme base de données principale pour les ensembles de données persistants ^78. Maintenant que nous comprenons la nécessité et les avantages d'une architecture de base de données découplée et que nous avons examiné certains cas d'utilisation que des bases de données spécialement conçues peuvent résoudre ^79. J'aimerais vous présenter Michaeli qui a mis tout cela en pratique sous la forme d'une démonstration à Michael ^80. Merci William ^81. Dans cette démo, nous allons examiner des animaux de compagnie géniaux, awesome pets est une entreprise fictive et dans ce scénario inventé, awesome pets est une animalerie en ligne, awesome pets possède toutes les caractéristiques d'une boutique de commerce électronique où vous pouvez parcourir le catalogue, ajouter des articles au panier et découvrir la technologie pour animaux de compagnie actuellement géniale La pile se compose d'un back-end monolithique existant et d'un ^82 monolithique. Ma base de données de suites ^83. Cette application monolithique intègre toutes les caractéristiques et fonctionnalités d'Awesome Pets ^84. Awesome Pets fait face à de nombreux défis avec leurs monolithes hérités ^85. Tout d'abord, ils ont découvert que certaines requêtes sont extrêmement lentes, en particulier en cas de charge élevée ^86. Ensuite, chaque requête devenant de plus en plus lente pendant les pics de trafic, les demandes des utilisateurs échouent et les utilisateurs ne peuvent pas effectuer les transactions ^87. Enfin, la base de données est à court de ressources et de Seigneur, ce qui peut provoquer le blocage de la base de données et l'indisponibilité de l'ensemble de l'application ^88. Si vous essayez également de dimensionner des applications monolithiques ^89. Ces défis peuvent vous sembler très familiers ^90. Maintenant, je vais vous montrer comment fonctionne l'ancienne application Awesome Pets en exécutant un simple test ^91. Ensuite, je vais vous montrer comment des animaux de compagnie géniaux peuvent être découplés grâce à des microservices et à des bases de données spécialement conçues ^92. Comment choisir la bonne base de données pour la bonne tâche et vous montrer les différences de performances entre l'application monolithique existante et l'application de microservices ^93. Enfin, j'aborderai brièvement certains des modèles de conception utilisés dans la nouvelle architecture ^94. Permettez-moi de commencer par vous montrer un exemple de parcours utilisateur sur des animaux de compagnie géniaux ^95. Comme vous pouvez le voir ici lorsque je visite des animaux géniaux ^96. On me présente le catalogue des animaux de compagnie ^97. Le catalogue indique les types d'animaux de compagnie disponibles dans les commentaires et rend compte de chaque type ^98. Je peux ajouter des animaux de compagnie au panier en cliquant sur le bouton Ajouter au panier. Une fois que j'ai ajouté tous les animaux, je peux regarder la carte et passer au paiement ^99. Sur la page de paiement ^100. Je peux saisir toutes les informations nécessaires pour terminer la transaction et confirmer le paiement. Une fois le message de confirmation affiché, le parcours utilisateur est terminé ^101. Nous allons donc maintenant exécuter le test de charge pour voir comment l'application monolithique se comporte sur la charge ^102. De nombreux outils sont disponibles pour exécuter des tests de performance tels que j meter ou artillerie ^103. Pour n'en nommer que quelques-uns ^104. Mais pour cette démo, je vais utiliser Blaze Meter ^105. Ce test simule un véritable parcours utilisateur sur des animaux de compagnie géniaux où chaque utilisateur recevra un catalogue d'un article au panier et en soumettra un autre ^106. Ce test est configuré avec un total de 50 utilisateurs simultanés naviguant dans le trajet pendant une durée de 10 minutes et le nombre d'utilisateurs augmentera progressivement sur une durée d'une minute ^107. Enfin, ce test sera exécuté dans la région AWS Oregon ^108. Je vais maintenant commencer le test de charge et comme le test dure environ 10 minutes, je vais accélérer la vidéo et nous examinerons les résultats une fois le test terminé ^109. Le test est donc maintenant terminé et nous pouvons regarder les résultats dans ces écrans, nous avons deux graphiques ^110. Celui de gauche indique le nombre de demandes réussies pendant la durée du test et le nombre de demandes ayant échoué ^111. Et le graphique de droite affiche le temps de réponse moyen sur la durée du test ^112. Si nous nous concentrons sur le graphique de droite, nous pouvons voir qu'à mesure que le nombre d'utilisateurs augmente, le temps de réponse moyen de l'application ne cesse d'augmenter, atteignant un temps de réponse maximal d'environ sept secondes ^113. Et à un moment donné, les temps de réponse ont chuté de façon drastique à quelques centaines de millisecondes seulement et après quelques minutes, le temps de réponse remonte à sept secondes ^114. Si nous regardons le graphique de gauche, nous voyons clairement pourquoi il y a une baisse du temps de réponse aux alentours de 1750 ^115. Nous pouvons constater qu'à l'heure actuelle, presque toutes les requêtes échouent et qu'il n'y a aucune demande réussie ^116. Cela nous indique que le site Web était en panne pendant cette période et que la demande de l'utilisateur a échoué très rapidement ^117. Allons donc un peu plus loin pour voir la cause de ces problèmes ici ^118. J'utilise AWS X ray, un service de suivi qui permet d'approfondir la demande ayant échoué à partir de la carte de service X ray ^119. Je peux voir mon application monolithique et les requêtes envoyées à la base de données ^120. La couleur rouge représente ici le nombre de requêtes qui ont échoué dans la base de données ^121. Je vais donc maintenant utiliser X Ray Insights, AWS X Ray Insights identifie où se produisent les problèmes dans les applications, les enregistrements de chaque problème et l'impact associé ^122. Et lorsque j'ouvre un aperçu, je peux voir immédiatement quelle est la cause première du problème et l'impact qu'il a. Dans ce cas, cela me dit que 23 % de la demande a échoué dans la base de données ^123. Ensuite, je peux cliquer sur Analyser les informations pour accéder à chaque demande échouée ^124. Une fois que j'ai ouvert une demande qui a échoué, je peux voir tous les composants qui ont été exécutés pour cette demande ainsi que la durée pendant laquelle chacun d'entre eux a pris ^125. Dans ce cas précis, la base de données a renvoyé une erreur au bout de 17 secondes en raison d'un blocage ^126. Enfin, examinons les métriques basées sur les données pour comprendre plus en détail ce qui s'est passé ^127. Si nous nous concentrons sur l'utilisation du processeur, nous pouvons constater que l'utilisation du processeur de la base de données atteint 100 % très rapidement et qu'après quelques minutes d'utilisation continue du processeur à 100 %, l'utilisation de l'adresse IP de la base de données revient à 0 %, puis elle revient à 100 % ^128. Cela est dû au fait que la base de données se bloque et redémarre en raison d'une utilisation élevée du processeur ^129. Vous pouvez également le valider en examinant d'autres indicateurs tels que right I ^130. Opérations et connexions à la base de données ^131. Comme nous l'avons vu, cette ancienne application ne fonctionne pas bien, les requêtes peuvent prendre jusqu'à 10 secondes pendant les pics de trafic et les pannes de base de données provoquant des pannes. Des animaux de compagnie géniaux pourraient envisager de mettre en œuvre de nombreuses optimisations courantes pour les applications monolithiques ^132. Cependant, ils prévoient de multiplier leur croissance par 10 au cours de la prochaine année et savent que même avec des optimisations communes pour les applications monolithiques, ils ne seront pas en mesure de soutenir la croissance prévue ^133. C'est pourquoi des animaux géniaux ont décidé de passer aux microservices avec des bases de données spécialement conçues ^134. Essayons donc de comprendre comment ils décomposent cette application, comment ils ont décidé quelle base de données utiliser pour chaque cas d'utilisation. Dans cette nouvelle architecture, awesome pets est divisé en quatre microservices, le panier de commande d'inventaire et le catalogue de l'inventaire ^135. Le service Micro utilise dynamodb ^136. Les données d'inventaire peuvent être stockées dans un format de valeur clé simple et il existe un schéma bien défini, tel que l'ajout ou la suppression d'animaux de compagnie à l'inventaire ^137. Ensuite, nous avons l'ancien traitement ^138. Ce cas d'utilisation nécessite la cohérence des données et stocke les données dans un format relationnel dénormalisé ^139. Certaines des logiques précédentes peuvent également être réutilisées et c'est pourquoi elle convient parfaitement à un Amazon Aurora ^140 compatible avec ma suite. Ensuite, nous avons les microservices Cast ^141. La fonctionnalité de casting est la deuxième fonctionnalité la plus utilisée sur Awesome Pets ^142. Son volume de lectures et d'écritures est élevé et les temps de réponse nécessaires devraient être de sept millisecondes, comme vous vous en souvenez peut-être lors de la session ^143 de Williams. Il s'agit d'un excellent cas d'utilisation pour une base de données en mémoire telle que celle de Brady sur Elastic Cache ^144. Enfin, nous avons le catalogue, le catalogue actuel, interroge tout l'inventaire disponible et gère un compte en fonction du type d'animal de compagnie, car Awesome Pets utilise une base de données relationnelle ^145. Ils ne sont actuellement pas en mesure de fournir des fonctionnalités de recherche riches et, par conséquent, l'une des technologies qu'ils peuvent utiliser pour le catalogue est le service Amazon Open Search ^146. La recherche ouverte permet d'assimiler facilement des recherches et d'agréger des milliards de documents ^147. Maintenant que nous avons vu à quoi ressemble l'architecture cible, lançons un autre test de performance pour voir comment fonctionne la nouvelle architecture. Comme vous pouvez le constater, je peux ajouter les paramètres au U ^148. OU ^149. L^150. Pour commencer à utiliser le backend des microservices et dans ces derniers, j'ai à peu près le même nombre d'animaux de compagnie, sinon plus, par rapport à l'application monolithique et les fonctionnalités sont exactement les mêmes ^151. Je peux donc commencer et ouvrir le test pour mon backend de microservices ^152. Comme vous pouvez le constater, le test de performance est identique à la même configuration que le test précédent et le parcours utilisateur est le même ^153. Nous allons récupérer le catalogue, ajouter une carte et soumettre une commande ^154. De plus, la configuration est la même avec 50 utilisateurs au total pour une durée maximale de 10 minutes et avec une augmentation d'une minute ^155. Alors allons-y, lançons ce nouveau test et examinons les résultats ^156. Une fois le test terminé ^157. Comme avant, je vais accélérer cette partie de la vidéo jusqu'à ce que le test soit terminé ^158. Examinons ces résultats en un coup d'œil ^159. Nous pouvons déjà constater que les performances sont beaucoup plus constantes qu'il n'y a pas eu d'erreur lors du test de performance ^160. Et dans l'ensemble, le temps de réponse moyen est d'environ 160 millisecondes, soit 45 fois plus rapide que notre application monolithique ^161. Jetons donc un coup d'œil à la radiographie et nous pouvons voir ici toutes les interactions entre les microservices ^162. Par exemple, nous pouvons voir comment la fonctionnalité d'envoi de toutes les fonctionnalités couvre réellement plusieurs microservices ^163. Et ici, nous pouvons voir les appels d'API pour obtenir le catalogue et ajouter des articles au panier ^164. Je peux maintenant examiner des traces spécifiques pour approfondir une seule demande de commande ici. Nous pouvons voir la répartition du temps passé pendant la durée de l'envoi ou de la transaction ^165. Nous pouvons le voir sous la charge ^166. Cette opération n'a pris qu'environ 300 millisecondes, soit 25 fois plus rapide que la soumission à la fonctionnalité de l'application monolithique dans les mêmes conditions. Dans la démo, nous avons vu comment l'architecture des microservices est capable de bien évoluer pour faire face au trafic et fournir des temps de réponse cohérents ^167. Une autre caractéristique de cette nouvelle architecture est qu'elle permet d'obtenir une disponibilité bien supérieure ^168. Si l'une des bases de données tombe en panne, un seul des microservices est affecté et l'application peut encore fonctionner partiellement en passant aux microservices ^169. Non seulement nous pouvons nous adapter et améliorer nos performances, mais nous pouvons également bénéficier d'une plus grande disponibilité ^170. Enfin, je voudrais mentionner brièvement les motifs de conception qui ont été utilisés dans cette composition ^171. La première est la notification d'événements dans cette nouvelle architecture ^172. Le microservice du catalogue doit être mis à jour chaque fois qu'un article est ajouté ou retiré de l'inventaire ^173. Pour implémenter ce modèle, nous utilisons les flux de dynamodb, qui est une fonctionnalité de dynamodb qui vous permet de diffuser chaque modification sur la table ^174. Le deuxième modèle est le modèle saga ^175. Ce modèle nous permet d'exécuter des transactions commerciales sur plusieurs microservices ^176. Dans cet exemple, le modèle de cycle a été implémenté à l'aide des fonctions d'étape AWS et la transaction couvre l'inventaire, la commande et les microservices cat ^177. Si vous souhaitez en savoir plus sur les modèles de microservices ^178. Je vous recommande de regarder la session intitulée Créer des microservices brésiliens à l'aide de modèles tolérants aux pannes. Au cours de cette session, nous avons examiné quelles sont les exigences actuelles des applications modernes ^179. Nous avons ensuite exploré les raisons pour lesquelles les clients sont passés d'une approche universelle à des bases de données personnalisées ^180. Nous avons examiné comment vous pouvez choisir la bonne base de données pour la bonne tâche et nous avons vu comment elle peut vous aider à atteindre une plus grande échelle, de meilleures performances et une plus grande disponibilité ^181. Nous avons abordé un certain nombre de sujets différents au cours de cette session, mais l'apprentissage ne doit pas s'arrêter là ^182. Nous vous encourageons à consulter notre contenu de formation et de certification ^183. Nous proposons plus de 500 cours numériques gratuits qui peuvent vous aider, vous et votre équipe, à acquérir de nouvelles compétences en matière de cloud et à découvrir les derniers services ^184. Et au fur et à mesure que vous développez vos compétences, pensez à vous préparer à l'une de nos 11 certifications aws ^185. Vous pouvez scanner les codes QR de cette diapositive pour en savoir plus ^186. Merci d'avoir participé à cette conférence ^187. Nous aimerions connaître vos commentaires pour nous aider à améliorer l'expérience du sommet AWS ^188. N'oubliez donc pas de répondre à l'enquête de session ^189. Merci ^190.