Hello and welcome to choose the right database for modern applications as part of our added West Summit online series. My name is William Wong and I'm a specialist database solutions architect. And today I'll be joined by Michael Riccardi, who's our specialist development solutions architect. Today. We're really excited to be here to show you how purpose built databases could be used to improve the scale performance and availability of your applications for our agenda will have an introduction of the requirements from a modern day applications. Will then look at the challenges that are solved using microservices and purpose built databases Before looking at how we can go about picking up the right databases for your particular workloads. Michael. We will then dive a bit deeper where he'll modernize an app from a monolithic relational database stack to that of using purpose built databases and show you the differences in both performance and scale. So let's get started and look at the requirements from our modern apps. And we first look at the apps that we each use every day like our ride sharing, media, streaming, banking, gaming and social media. We begin to see some common patterns. These patterns could be millions of users accessing our apps across different geographies. And these same users are expecting instant experiences which could boil down to consistent millisecond or even sub millisecond response times. Our apps will need to hyper scale to meet demands for events such as our flash sales or urine processing and then scale back down. We're not in use so that we can minimize costs. It is anticipated that will capture more data in the next three years than that of the last 30 years. And this will need specialized tools to process the petabytes. If not zero bytes worth of structured and unstructured data. If we take some time now and think about the architectural patterns of most of these cloud base apps, you'll find that they are all microservices. That means they are highly distributed, loosely coupled and accessed via a P. I. S. So what does that change in architecture mean to our underlying database As our apps are now decoupled into services. It allows each of these services to have its own independent database. And that'll bring us a lot of advantages, for example, will give us the ability to hyper scale our applications as each service and database canal scale independently. Let's take black friday as a use case. We might need to scale our catalog and payment service to hundreds of thousands of concurrent requests over a short time frame but our user registrations may not grow to that same degree. Another benefit is the added agility. Since we can now innovate faster across different components. We can quickly test and roll back new builds and features on a modular level without dealing with all the complex coupled dependencies which are related to monoliths. A common business challenge is making our apps more available and by decoupling our databases, it will increase its overall availability since we no longer have a monolithic database which serves as that single point of contention for our events such as code deployments or upgrades and patches. So now that we understand the advantages of decoupling our data. T why would we consider purpose built databases for our microservices? Our developers want the right database to meet the needs of our modern applications as we described earlier. And frankly that one size fits all approach of using a relational database for everything, just no longer works. For example, we might need a database to provide microsecond latency response times so that we can quickly render our websites or to deliver consistent response times to cater for a surgeon user demands. And the relational architecture is not the best fit for these particular use cases. In fact, by not using the right database will typically lead to performance issues, lack of scalability, lack of developer flexibility and an increase to our overall costs. So traditionally, when I talk to my customers a barrier for adopting these purpose built databases with a potential operational overheads. And this included investments in upfront hardware and software or expertise to make them scalable, highly available and performant. And this is where aws comes in. We'll offer the broadest and deepest portfolio of 15 plus purpose built databases that can support the different data models by leveraging, purpose built and fully managed databases that we built from the ground up customers can now save time and costs, improve performance at scale and innovate faster. We have purpose built databases to match every use case like relational key value document in memory, wide column time series ledger and our graph databases. So now that we understand the need for purpose built databases, how then would you go about selecting the right databases for your particular use cases? What I'd like to tell people is instead of looking at a list of 100 different databases, why don't we start off by thinking about common database categories? So now we quickly look across the categories. Not only would you find that familiar relational database there on the left, but you'll also see other databases, databases like our amazon document DB which is optimized for storing the data in Jason format. And since it's a no sequel database, it will give us that flexibility around changing application schemers but it will also allow us to query documents based on any attribute. And this is really handy for our content management or mobile apps or a graph database like amazon Neptune, which then allows us to work with highly connected data sets. We can try to model this out in a relational database with complicated joints and nested queries but our latency is will grow as a number of our relationships increase in graph databases. However, it allows us to traverse millions of relationships in seconds, which is great for fraud detection, social media and our recommendation engines. Well lastly like a time series database like our amazon time stream which is optimized to ingest trillions of time sequence data a day as well as giving us time based functions such as correlation and interpolation so that we can gain better insights from that data. And this is great for our IOT devops or event tracking apps. Let's take some time now to dive a little further into some very common categories that I've seen amongst my customers. Let's start off with a really familiar relational data model, relational data is highly structured and the data is broken up into tables and the relationships enforced by the system are primary and referential keys and good use cases for them will be workloads where we can't predefined all our access patterns up front or if we had apps that requires high referential integrity and strong consistency. Like for our online payment systems. A cloud native database we could choose for our relational models is amazon Arora. Arora is compatible with both my sequel and Postgres and can help us enhance our performance by providing up to five times more throughput than that of standard. My sequel and three times more than that of standard Postgres. It will help us automatically scale both compute and storage resources and we drew billy store our data six ways across three availability zones. And since it's a managed service will provide for automation for tasks such as deployment and provisioning regular patching and upgrades, backups and give us security features like encryption in transit and at rest. Another great category. To look at our our key value databases. Key value data is one that uses that simple key value method to store and retrieve data and its strength lies behind its design to highly partition or shard data and then physically store it based on that partition key. That design allows it to horizontally scale to virtually any size while still giving us consistent response times, irrespective of scale. So let's go through a use case for an online game that needs to store a user's session data and has a defined access pattern through gamer tag for retrieving that data set. Since it's an online game, we want to ensure a consistent experience back to the gamer irrespective of if there were 10 or even 100,000 users. If the game takes off in this particular use case, we'll use a key value database to achieve consistent response times, irrespective of the growth of the application. A purpose built database we could choose for our key value databases is amazon dynamodb. It is fully managed and serverless, meaning that it will take care of all the provisioning software, patching security and to help us automatically scale dynamodb enables us to build apps that can provide for single digit millisecond response times at scale and for our mission critical apps we could use global replication to replicate our data sets across multiple regions. A really popular category amongst my customers are in memory databases In memory databases will store our data in memory rather than on this, which then gives us sub millisecond response times. And that is great for use cases to store a user session data on maintaining a leaderboard for online games or training machine learning models. But the most common use case that I've seen amongst my customers is placed here as a cash in front of our relational databases. This will give us much better response times to our end users and alleviates read scaling pressures on the underlying relational database which can lead to locking resource contention and stability issues, which Makayla will show you in his subsequent demonstration for in memory databases. We have the choice of amazon elastic cache. It is fully managed service that allows us to provide for sub millisecond response times and non disruptive scaling. It is both compatible with mem cache. DM Reedus and people were using it for mem cache will typically use it to build simple scaling cation layers for their latency sensitive apps whereas our Reddit users, they'll use it for more versatile use cases like for their gaming real time analytics and machine learning with regis. We also have the choice of amazon Memory Day bay which is a fully managed cluster with durability. Memory debate provides us with ultrafast performance with full compatibility with open source regis which then allows us to use all of this is rich data structures such as sorted sets for online leaderboards by leveraging. A distributed transaction log across availability zones. Memory DB provides us with fast database recovery and restarts without the risk of data loss, meaning that we can now use this as our primary database for persistent data sets. So now that we understand the need and benefits of a decoupled database architecture and have gone through some use cases that particular purpose built databases solves. I'd like to introduce you to Michaeli who put all this into action in the form of a demonstration over to Michael. Thank you William. In this demo we will look at awesome pets, awesome pets is a fictitious company and in this made up scenario, awesome pets is an online pet store, awesome pets has all of the characteristics of an e commerce store where you can browse the catalog, add items to the basket and check out currently awesome pets tech stack consists of a legacy monolithic back end and a monolithic. My sequel database. This monolithic application powers all the different features and functionality of awesome pets. Awesome pets is facing a number of challenges with their legacy monoliths. First, they have found that some queries are painfully slow, especially in cases of high load. Then, as a result of each query getting slower and slower during peak traffic, user requests are failing and users are unable to complete transactions. Lastly, the database runs out of resources and the Lord which can cause the database to crash and the entire application to be unavailable. If you are also trying to scale monolithic applications. These challenges may sound very familiar to you. So now I will show you how the legacy awesome pets application performs by running a simple test. Then I will show you how awesome pets can be decoupled with microservices and purpose built databases. How to pick the right database for the right job and show you the differences in performance between the legacy monolithic application and microservices application. Lastly, I will briefly touch on some of the design patterns used in the new architecture. Let me start by showing you a sample user journey on awesome pets. As you can see here when I visit awesome pets. I am presented with the pets catalog. The catalog shows the pet types available in the commentary and account for each type. I can add pets to the cart by clicking the add to cart button and once I added all of the pets, I can take a look at the card and proceed to the checkout. In the checkout page. I can enter all the details required to complete the transaction and confirm the payment and once the confirmation message is displayed, user journey is complete. So let's now run the load test to see how the monolithic application performs on the load. There are many tools available to run performance tests such as j meter or artillery. To name a few. But for this demo, I'm going to use blaze meter. This test simulates a real user journey on awesome pets where each user will get a catalog at an item to the cart and submit another. This test is configured with a total of 50 concurrent users navigating through the journey for a duration of 10 minutes And the number of users will ramp up gradually over the duration of one minute. Lastly, this test will run in the aws Oregon region. I will now start the load test and as the test takes about 10 minutes to complete, I will speed up the video and we will take a look at the results once the test is complete. So the test is now complete and we can take a look at the results in these screens, we have two graphs. The one on the left shows how many requests over the duration of the test succeeded and how many requests failed. And the graph on the right displays the average response time over the duration of the test. If we focus on the graph on the right hand side, we can see that as the number of users ramps up, the average response time of the application keeps increasing, reaching a peak response time of about seven seconds. And at some point, the response times dropped drastically to only a few hundreds of milliseconds And after a few minutes, the response time goes back up to seven seconds. If we look at the graph on the left, it's clear why there is a drop in response time at around 1750. We can see that at this time almost all of the requests fail and there are no successful requests. This tells us that the website was down during that time and the user request was failing very quickly. So let's go a bit deeper to see what's causing these issues here. I am using AWS X ray which is a tracing service to dive deeper into the failed request from the X ray service map. I can see my monolithic application and the requests going to the database. The red color here represents the amount of requests that have failed in the database. So I'm now going to use X ray insights, AWS x ray insights identifies where in the applications issues are occurring, the records of each issue and the associated impact. And when I open an insight I can see immediately what is the root cause of the issue and the impact it's having In this case it's telling me that 23% of the request failed in the database. Then I can click on analyze insight to drill down to each individual failed request. Once I opened a failed request, I can see all of the components that executed for this request as well as how long each one of them took. In this specific case the database returned an error after 17 seconds due to a deadlock. Lastly, let's take a look at the data based metrics to understand in more details what happened. If we focus on the CPU utilization, we can see that the database CPU utilization spikes up to 100% very quickly and after a few minutes of sustained 100% CPU utilization, the database iP utilization goes back to 0% And then it comes back up to 100%. This is because the database crashes and restarts due to high CPU utilization. You can also validate this by looking at other metrics such as right I. Ops and database connections. As we've seen this legacy application does not perform well, requests can take up to 10 seconds during the traffic peaks and the database crashes causing outages awesome pets could look at implementing many of the common optimization for monolithic applications. However, they expect to grow 10 times over the next year and they know that even with common optimizations for monolithic applications, they will not be able to support their planned growth. This is why awesome pets decided to move to microservices with purpose built databases. So let's try to understand how they decompose this application, how they decided which database to use for each use case in this new architecture, awesome pets is split into four microservices, the inventory order cart and catalog the inventory. Micro service uses dynamodb. The inventory data can be stored in a simple key value format and there is a well defined pattern such as adding or removing pets to the inventory. Then we have the older processing. This use case requires data consistency and stores data in a de normalized relational format. Some of the previous logic can also be reused and this is why it's a great fit for a my sequel compatible amazon aurora. Then we have the cast microservices. The cast functionality is the second most used functionality on awesome pets. It has high volume of reads and writes and the response times needed should be in seven milliseconds as you may remember from Williams session. This is a great use case for an in memory database such as brady's on elastic cache. And lastly we have the catalog, the catalog at the moment, queries all of the available inventory and performs account based on the pet type because awesome pets utilizes a relational database. They are currently not able to provide rich search functionality and therefore one of the technologies that they can use for the catalog is the amazon Open Search service. Open search can easily ingest search and aggregate billions of documents. So now that we've seen how the target architecture looks like, let's run another performance test to see how the new architecture performs as you can see I can add the parameters to the U. R. L. To start using the microservices back end and in these microservices back end, I have around the same number of pets if not more compared to the monolithic application and the functionality is exactly the same. So I can go ahead and open the test for my microservices back end. As you can see, the performance test is the same as the same configuration as the previous test and the user journey is the same. We're getting the catalog, adding a card and submit an order. And also the configuration is the same with 50 total users for up to 10 minutes duration and with among one minute ramp up. So let's go ahead and run this new test and take a look at the results. Once the test is complete. Like before I will speed up this part of the video until the test completes. Let's take a look at these results at a glance. We can already see that the performance is a lot more consistent that there were no errors during the performance test. And overall the average response time is around 160 milliseconds, which is 45 times faster than our monolithic application. So let's take a look at the X ray and here we can see all of the interactions between the microservices. For example, we can see how the submit all the functionality actually spans multiple microservices. And here we can see the api calls for getting the catalog and adding items to the cart. I can now look at specific traces to dive deeper on a single submit order request here, we can see the breakdown of where the time was spent for the duration of the submit or the transaction. We can see that under load. This operation only took about 300 milliseconds, which is 25 times faster that the submit to the functionality in the monolithic application under the same conditions in the demo, we saw how the microservices architecture is able to scale well to cope with the traffic and provide consistent response times. Another characteristic of this new architecture is they can achieve much greater availability. If one of the databases fails, only one of the microservices is affected and the application can partially still function by moving to microservices. Not only we can scale and perform better, but we also gain greater availability. Lastly, I'd like to briefly mention to design patterns that were used in this composition. The first is event notification in this new architecture. The catalog micro service needs to be updated whenever there is an item added or removed from the inventory. To implement this pattern, we leverage dynamodb streams, which is a feature of dynamodb that allows you to stream every change on the table. The second pattern is the saga pattern. This pattern allows us to execute business transactions across multiple microservices. In this example, the cycle pattern was implemented using AWS step functions and the transaction spans the inventory, the order and the cat microservices. If you'd like to find out more about microservices patterns. I'd recommend watching the session named build Brazilian microservices using fault tolerant patterns in this session, we looked at what are today's modern application requirements. We then explored why customers moved from a one size fits all approach to purpose built databases. We looked at how you can choose the right database for the right job and we saw how it can help you achieve greater scale, better performance and greater availability. We covered a number of different topics in this session, but the learning does not have to stop here. We encourage you to check out our training and certification content. We offer over 500 free digital courses that can help you and your team build new cloud skills and learn about the latest services. And as you build your skills, consider preparing for one of our 11 aws certifications. You can scan the QR codes in this slide to find out more. Thank you for attending this talk. We would love to hear your feedback to help us improve the AWS summit experience. So please remember to complete the session survey. Thank you